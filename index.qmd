---
title: "Writing a great story for data science projects - Spring 2026 "
subtitle: "Report Template Quarto - Alexis B."
author: "Alexis Bjornstad (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!

Nice report!
:::

## Introduction

The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.

<!-- -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Source 1 Summary - https://www.sciencedirect.com/science/article/pii/S0022169420309914?casa_token=kzXuK-KR6Q0AAAAA:XxH1CSWUThtWAHCdrmOfKvDuT7pEDgwYCMu8aLtwEkqRaFVR3NFQw-FRoZTSKwLZ6mUzQ1s 
The article described an analysis comparing a hydrological mapping model with a random forest model for mapping large-scale flood simulations. The study described how the random forest technique was optimal for performing hydrological studies due to the ability of the machine learning model to map flood risk analysis for hazard assessment with a low-cost application of the non-traditional model compared to rainfall-runoff models. The flood simulation focuses on water basins in Canada and the US with low to medium magnitude floods in low altitudes with warm temperatures. The study found that the conditional discharge rate in the random forest model differed from the mean of conditional discharge. However, the random forest model was able to reproduce the characteristics of the studied flood events and represented the daily flood discharge well in comparison with the hydromad model. The higher magnitude of flooring deterioration affected the modelâ€™s accuracy, with a slight preference towards the hydromad model. The study concludes that the random forest model is a promising contender for use in flood discharge and risk, but may require further improvements before being implemented in higher-magnitude flooding scenarios.

Source 2 Summary - https://link.springer.com/article/10.1007/s00521-021-05757-6 
This article assesses the risks of flood and its application to hazard management with the random forest algorithm application. The flood disaster risk is weighed within an index of a flood risk assessment that ArcGIS integrated into the model. The decision tree classifiers from the random forest algorithm are used to vote on the classifiers of flood risk. The study utilizes a combination of remote sensing, geographical, and statistical data to model the disaster risk. Five levels of risk are created to define disaster loss variability. The study concludes that the random forest application to the time series flood risk data is useful for large-scale disaster evaluation. The study emphasizes human loss and property damage as the most important indicators of flood risk analysis (within their research). 




Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine
learning [@adams2021topology]

For Symbolic Regression [@wang2019symbolic] *This is my work and I want
to add more work...*

Cite new paper [@su2012linear]

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
#import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
