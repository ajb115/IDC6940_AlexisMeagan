---
title: "Writing a great story for data science projects - Spring 2026 "
subtitle: "Report Template Quarto - Alexis B."
author: "Alexis Bjornstad (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!

Nice report!
:::

## Introduction

The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.

<!-- -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Source 1 Summary -
https://www.sciencedirect.com/science/article/pii/S0022169420309914?casa_token=kzXuK-KR6Q0AAAAA:XxH1CSWUThtWAHCdrmOfKvDuT7pEDgwYCMu8aLtwEkqRaFVR3NFQw-FRoZTSKwLZ6mUzQ1s
The article described an analysis comparing a hydrological mapping model
with a random forest model for mapping large-scale flood simulations.
The study described how the random forest technique was optimal for
performing hydrological studies due to the ability of the machine
learning model to map flood risk analysis for hazard assessment with a
low-cost application of the non-traditional model compared to
rainfall-runoff models. The flood simulation focuses on water basins in
Canada and the US with low to medium magnitude floods in low altitudes
with warm temperatures. The study found that the conditional discharge
rate in the random forest model differed from the mean of conditional
discharge. However, the random forest model was able to reproduce the
characteristics of the studied flood events and represented the daily
flood discharge well in comparison with the hydromad model. The higher
magnitude of flooring deterioration affected the model’s accuracy, with
a slight preference towards the hydromad model. The study concludes that
the random forest model is a promising contender for use in flood
discharge and risk, but may require further improvements before being
implemented in higher-magnitude flooding scenarios.

Source 2 Summary -
https://link.springer.com/article/10.1007/s00521-021-05757-6 This
article assesses the risks of flood and its application to hazard
management with the random forest algorithm application. The flood
disaster risk is weighed within an index of a flood risk assessment that
ArcGIS integrated into the model. The decision tree classifiers from the
random forest algorithm are used to vote on the classifiers of flood
risk. The study utilizes a combination of remote sensing, geographical,
and statistical data to model the disaster risk. Five levels of risk are
created to define disaster loss variability. The study concludes that
the random forest application to the time series flood risk data is
useful for large-scale disaster evaluation. The study emphasizes human
loss and property damage as the most important indicators of flood risk
analysis (within their research).

Source 3 Summary - https://www.sciencedirect.com/science/article/pii/S0022169400002730?casa_token=8vWeO8P5XVMAAAAA:C5OIQ0yI14ryLeaRuyGLcRvgBRaLNbq1w08MyHDkHOy9cxGCw2LwbY0zqakT92cEPCA9rNY 
This article assesses the Gulf States and the Florida region, and the Carolinas, to show that spatial homogeneity in the main clusters of the southeastern US analysis. 	The article summarizes the flood regime and patterns of each region and the percentage of annual floods that occur within each year. The article also takes into account the watershed size and monthly flood frequencies for comparison. Flood-generating events are producers of precipitation with varying intensities and magnitudes. The hydroclimatology process represents the relationships of the corresponding atmospheric processes and the flood timing and intensity. The article uses the USGS’s Hydro-Climatic Data Network to assess the 1950s through the 1990 record of gaging stations that describe flood events. The gaging stations' locations and the drainage are represented on a map of the southeastern United States, and the clustering algorithm was used to create distinctions from cluster to cluster. The cluster frequencies and peaks were shown to represent the flood frequencies considering drainage, spatial homogeneity, seasonal occurrences, and regional averages. Each region represented highly clustered areas and the dominating seasonal patterns of flooding in the area. Hierarchical clustering was used to rank distinct regions of classification. Flood potential is affected by geology, soil, vegetation, topography, and antecedent conditions, as well as the hydroclitaolgy studied in the paper. 

Source 4 Summary - https://www.sciencedirect.com/science/article/pii/S0022169415004217?casa_token=kffvjbeRUOIAAAAA:YPd0wUWWfgEqbhsRSr2xfQVCL6usyb02_nqU-VS7D5uSEGcRUYYMTin89uoZThWZyqCzWKg 
This article focuses on the flood risk assessment using random forest to run on a large data set and considers variables that factor into risk assessment (non-linear relationship approach). The study of RF application is applied to 11 risk indices and 5,000 samples in training and testing for the Dongjinag River Basin, China. The errors produced by the random forest training can be counteracted by increasing the number of samples and trees of classification. The high-risk zones are characterized and occupy over 19% of the total area of the flood region assessment. The index includes the 3 day precipitation, the runoff depth, the frequency of typhoons, elevation, and wetness as the top five considerations out of the 11 assessed (71% of total risk). The risk level of the samples contained 5 levels from lowest risk to highest risk. The RF application allowed for the importance of each variable to be assessed, and each index contribution to the overall risk was calculated with the Gini decrease index. The rainfall was considered the primary source of flood hazards and was supported in the random forest assessment. The article’s conclusion supported the application of random forest modeling when conducting and assessing flood risks and hazards. 




Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine
learning [@adams2021topology]

For Symbolic Regression [@wang2019symbolic] *This is my work and I want
to add more work...*

Cite new paper [@su2012linear]

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
#import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
